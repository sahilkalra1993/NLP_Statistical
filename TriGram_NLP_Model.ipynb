{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TriGram_NLP_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgQZZ-ztL6f-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "da337a59-1dc3-4206-d00e-5e8a21aa6885"
      },
      "source": [
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaIsfD3NIEUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code courtesy of https://nlpforhackers.io/language-models/\n",
        "# https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/\n",
        "\n",
        "# The Reuters Corpus contains 10,788 news documents totaling 1.3 million words. \n",
        "# The documents have been classified into 90 topics, and grouped into two sets, \n",
        "# called \"training\" and \"test\"; thus, the text with fileid 'test/14826' is a \n",
        "# document drawn from the test set. This split is for training and testing \n",
        "# algorithms that automatically detect the topic of a document, as we will see \n",
        "# in chap-data-intensive.\n",
        "\n",
        "from nltk.corpus import reuters\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG2TNu7mIPGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a placeholder for model\n",
        "# if the key not in dictionary, return a defaultdictionary which can return\n",
        "# 0 if queried for a non exiting item\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Example:\n",
        "# model['Sahil'] - defaultdict(<function __main__.<lambda>.<locals>.<lambda>>, {'Kalra': 0})\n",
        "# model['Sahil']['Kalra'] - 0\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK1-pZ4zN_T5",
        "colab_type": "text"
      },
      "source": [
        "We first split our text into trigrams with the help of NLTK and then calculate the frequency in which each combination of the trigrams occurs in the dataset.\n",
        "\n",
        "We then use it to calculate probabilities of a word, given the previous two words. Thatâ€™s essentially what gives us our Language Model!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H4Cbj5bK0V-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Count frequency of co-occurance using trigrams\n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model[(w1, w2)][w3] += 1\n",
        "\n",
        "# Transform the counts to probabilities\n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8VXtiZTh6iV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dict(model[(\"the\", \"price\")]) <-- To see all the available predictions\n",
        "\n",
        "def predict(model, list_words):\n",
        "    max_prob = 0\n",
        "    max_key = 0\n",
        "    for k,v in dict(model[list_words]).items():\n",
        "        if v>max_prob:\n",
        "            max_prob=v\n",
        "            max_key=k\n",
        "        \n",
        "    print (\"Prediction of next word: \", max_key, max_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5puFm3MMh-CT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3d0123d-18cf-4658-e945-051b0a10c961"
      },
      "source": [
        "predict(model, (\"the\", \"price\"))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction of next word:  of 0.3209302325581395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYno_JxILuod",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "16155c8f-f9e1-45ed-bfdd-932ae3e8ab56"
      },
      "source": [
        "# Iteratively running the above model to generate next 10 words.\n",
        "\n",
        "import random\n",
        "\n",
        "# starting words\n",
        "text = [\"today\", \"the\"]\n",
        "sentence_finished = False\n",
        "\n",
        "# Sentence is finished when 2 consecutive None encountered\n",
        "while not sentence_finished:\n",
        "    # select a random probability threshold  \n",
        "    r = random.random()\n",
        "    accumulator = .0\n",
        "\n",
        "    for word in model[tuple(text[-2:])].keys():\n",
        "        accumulator += model[tuple(text[-2:])][word]\n",
        "        # select words that are above the probability threshold\n",
        "        if accumulator >= r:\n",
        "            text.append(word)\n",
        "            break\n",
        "\n",
        "        if text[-2:] == [None, None]:\n",
        "            sentence_finished = True\n",
        " \n",
        "# Last 3 we do not want to print, as 3rd last and 2nd last is None. and last is\n",
        "# the new sentence starting.\n",
        "print (' '.join([t for t in text[:-2] if t]))\n",
        "\n",
        "# As we can see the 3rd and 2nd last items are None, so sentence stopped.\n",
        "text"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "today the emirate ' s forests .\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['today', 'the', 'emirate', \"'\", 's', 'forests', '.', None, None, 'The']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPyG5IbROC49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}